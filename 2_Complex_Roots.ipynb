{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88db23ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor                  \n",
    "import torch.nn as nn                     \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from tqdm import tqdm\n",
    "print(device)\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f70a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for complex roots\n",
    "class NNCR(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() \n",
    "              \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]).to(device)\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "    def forward(self,x):\n",
    "            if torch.is_tensor(x) !=True:\n",
    "                x= torch.from_numpy(x).to(device)\n",
    "            sigma = x.type(torch.DoubleTensor).to(device)\n",
    "            for i in range(len(layers)-2):\n",
    "                z = self.linears[i](sigma)\n",
    "                sigma = self.activation(z)\n",
    "            sigma = self.linears[-1](sigma)\n",
    "            return sigma\n",
    "\n",
    "    \n",
    "    #Modify the loss function as per the problem\n",
    "    def loss_func(self, x_train1):                     \n",
    "        g = x_train1.clone()\n",
    "                        \n",
    "        g.requires_grad = True\n",
    "        \n",
    "        u = self.forward(g)\n",
    "        real,imag = com_eqn(u,g)        \n",
    "        return (real**2).mean()+(imag**2).mean()\n",
    "        \n",
    "                                           \n",
    "    def closure(self,steps,eps=1e-8,lr=1e-1,show=True):\n",
    "            start = time.time()\n",
    "            optimizer = torch.optim.Adam(self.parameters(),lr=lr)\n",
    "            for i in tqdm(range(steps)):\n",
    "                loss = self.loss_func(x_train1)\n",
    "                self.mse = loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #Learning rate scheduling. It performs better using this even for Adam.\n",
    "                if i%(steps/4)==0:\n",
    "                    lr=lr/5\n",
    "                    optimizer = torch.optim.Adam(self.parameters(),lr=lr)\n",
    "                    if show==True:\n",
    "                        with torch.no_grad():\n",
    "                            print('Iter: ',i,'Loss: ',loss.detach().cpu().numpy(),' lr: ',lr)\n",
    "                if self.mse<=eps:\n",
    "                    print('Converged !')\n",
    "                    break\n",
    "            print('MSE Loss: ',float(loss.detach().cpu().numpy()))\n",
    "            print('total time: ',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9e3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quadratic Equations\n",
    "def com_eqn(u,x_train1):\n",
    "    x = u[:,[0]]\n",
    "    y = u[:,[1]]\n",
    "    b = x_train1[:,[0]]\n",
    "    c = x_train1[:,[1]]\n",
    "    return (x**2-y**2+b*x+c),(2*x*y+b*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5206bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250000, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make input data\n",
    "b = np.linspace(-5,5,500)\n",
    "c = np.linspace(-5,5,500)\n",
    "B,C = np.meshgrid(b,c)\n",
    "x_train1 = torch.from_numpy(np.hstack(( B.flatten()[:,None],C.flatten()[:,None]))).to(device)\n",
    "x_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb4a01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 7/10000 [00:00<06:28, 25.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  0 Loss:  12.375783004329111  lr:  0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▌                            | 2509/10000 [00:43<02:09, 57.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2500 Loss:  0.006408895225553652  lr:  0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████                   | 5011/10000 [01:26<01:26, 57.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  5000 Loss:  0.0017655809424622707  lr:  0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████▌         | 7507/10000 [02:09<00:42, 58.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  7500 Loss:  0.0007208401733932557  lr:  0.00016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10000/10000 [02:51<00:00, 58.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss:  0.0005287277058215822\n",
      "total time:  171.94635891914368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "steps=10000\n",
    "layers = np.array([2,30,30,30,30,2])\n",
    "Root_NN2 = NNCR(layers)\n",
    "Root_NN2.to(device)\n",
    "Root_NN2.closure(steps=steps,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0fe2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.9869e+00, -1.0042e+00],\n",
      "         [ 1.0000e+00, -1.7314e+00],\n",
      "         [-4.9917e-01, -1.6562e+00],\n",
      "         [-5.8320e-01,  4.4085e-04],\n",
      "         [-7.5040e-01, -1.9853e+00]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#torch.save(Root_NN2,'models/Complex_quad.pth')\n",
    "test=np.array([[-4,5],[-2,4],[1,3],[4,2],[1.5,4.5]])[None,:]\n",
    "print(Root_NN2(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fae399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
