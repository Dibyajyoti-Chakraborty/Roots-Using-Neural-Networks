{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c363dcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor                  \n",
    "import torch.nn as nn                     \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from tqdm import tqdm\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d70e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for real roots\n",
    "class NNR(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() \n",
    "              \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]).to(device)\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "    def forward(self,x):\n",
    "            if torch.is_tensor(x) !=True:\n",
    "                x= torch.from_numpy(x).to(device)\n",
    "            sigma = x.type(torch.DoubleTensor).to(device)\n",
    "            for i in range(len(layers)-2):\n",
    "                z = self.linears[i](sigma)\n",
    "                sigma = self.activation(z)\n",
    "            sigma = self.linears[-1](sigma)\n",
    "            return sigma\n",
    "    \n",
    "    #Modify the loss function as per the problem\n",
    "    def loss_func(self, x_train,n):                     \n",
    "        g = x_train.clone()\n",
    "                        \n",
    "        g.requires_grad = True\n",
    "        \n",
    "        u = self.forward(g)\n",
    "                \n",
    "        loss_f = self.loss_function(eqn(u,x_train,n),torch.tensor(0).type(torch.DoubleTensor).to(device))   \n",
    "        return loss_f\n",
    "                                           \n",
    "    def closure(self,steps,n,eps=1e-8,lr=1e-2,show=True):\n",
    "            start = time.time()\n",
    "            optimizer = torch.optim.Adam(self.parameters(),lr=lr)\n",
    "            for i in tqdm(range(steps)):\n",
    "                loss = self.loss_func(x_train,n)\n",
    "                self.mse = loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #Learning rate scheduling. It performs better using this even for Adam.\n",
    "                if i%(steps/4)==0:\n",
    "                    lr=lr/5\n",
    "                    optimizer = torch.optim.Adam(self.parameters(),lr=lr)\n",
    "                    if show==True:\n",
    "                        with torch.no_grad():\n",
    "                            print('Iter: ',i,'Loss: ',loss.detach().cpu().numpy(),' lr: ',lr)\n",
    "                if self.mse<=eps:\n",
    "                    print('Converged !')\n",
    "                    break\n",
    "            print('MSE Loss: ',float(self.mse.detach().cpu()))\n",
    "            print('total time: ',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843ae04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm for dividing polynomials\n",
    "def poly_div(dividend, divisor):\n",
    "    n = len(dividend)\n",
    "    m = len(divisor)\n",
    "    quotient = [0] * (n - m + 1)\n",
    "    remainder = dividend.copy()\n",
    "    for i in range(n - m, -1, -1):\n",
    "        quotient[i] = remainder[m + i - 1] / divisor[m - 1]\n",
    "        for j in range(m):\n",
    "            remainder[i + j] -= quotient[i] * divisor[j]\n",
    "    return quotient, remainder[:m - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82af4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eqn(u,x_train,n):\n",
    "    pol=torch.pow(u,n)\n",
    "    for i in range(n):\n",
    "        pol=pol+x_train[:,[i]]*torch.pow(u,n-i-1)\n",
    "    return pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d146b27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([125000, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3rd order\n",
    "b = np.linspace(-2,2,50)\n",
    "c = np.linspace(-2,2,50)\n",
    "d = np.linspace(-2,2,50)\n",
    "\n",
    "B,C,D = np.meshgrid(b,c,d)\n",
    "x_train = torch.from_numpy(np.hstack(( B.flatten()[:,None],C.flatten()[:,None],\n",
    "                                      D.flatten()[:,None]))).to(device)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "208fd619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/10000 [00:00<?, ?it/s]/home/djc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([125000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|                                         | 9/10000 [00:00<04:44, 35.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  0 Loss:  1.4697214930321882  lr:  0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▌                            | 2508/10000 [00:34<01:42, 73.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2500 Loss:  0.00259279587785947  lr:  0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████                   | 5009/10000 [01:09<01:08, 73.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  5000 Loss:  0.0009064353023678338  lr:  8e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████▌         | 7513/10000 [01:43<00:33, 73.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  7500 Loss:  0.0006891116390388397  lr:  1.6000000000000003e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10000/10000 [02:16<00:00, 73.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss:  0.0005921365839955234\n",
      "total time:  136.81823348999023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "steps=10000\n",
    "layers = np.array([3,20,20,20,20,20,20,20,20,1])\n",
    "Root_NN3 = NNR(layers)\n",
    "Root_NN3.to(device)\n",
    "Root_NN3.closure(steps=steps,n=3,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8b4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Root_NN3,'models/Cubic.pt')\n",
    "del Root_NN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726e821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for complex roots\n",
    "class NNCR(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() \n",
    "              \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]).to(device)\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "    def forward(self,x):\n",
    "            if torch.is_tensor(x) !=True:\n",
    "                x= torch.from_numpy(x).to(device)\n",
    "            sigma = x.type(torch.DoubleTensor).to(device)\n",
    "            for i in range(len(layers)-2):\n",
    "                z = self.linears[i](sigma)\n",
    "                sigma = self.activation(z)\n",
    "            sigma = self.linears[-1](sigma)\n",
    "            return sigma\n",
    "\n",
    "    \n",
    "    #Modify the loss function as per the problem\n",
    "    def loss_func(self, x_train1):                     \n",
    "        g = x_train1.clone()\n",
    "                        \n",
    "        g.requires_grad = True\n",
    "        \n",
    "        u = self.forward(g)\n",
    "        real,imag = com_eqn(u,x_train1)        \n",
    "        return (real**2).mean()+(imag**2).mean()\n",
    "\n",
    "                                           \n",
    "    def closure(self,steps,eps=1e-8,lr=1e-1,show=True):\n",
    "            start = time.time()\n",
    "            optimizer = torch.optim.Adam(self.parameters(),lr=lr)\n",
    "            for i in tqdm(range(steps)):\n",
    "                loss = self.loss_func(x_train1)\n",
    "                self.mse = loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #Learning rate scheduling. It performs better using this even for Adam.\n",
    "                if i%(steps/4)==0:\n",
    "                    lr=lr/5\n",
    "                    optimizer = torch.optim.Adam(self.parameters(),lr=lr)\n",
    "                    if show==True:\n",
    "                        with torch.no_grad():\n",
    "                            print('Iter: ',i,'Loss: ',loss.detach().cpu().numpy(),' lr: ',lr)\n",
    "                if self.mse<=eps:\n",
    "                    print('Converged !')\n",
    "                    break\n",
    "            print('MSE Loss: ',float(loss.detach().cpu().numpy()))\n",
    "            print('total time: ',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de414155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def com_eqn(u,x_train1):\n",
    "    x = u[:,[0]]\n",
    "    y= u[:,[1]]\n",
    "    b=x_train1[:,[0]]\n",
    "    c=x_train1[:,[1]]\n",
    "    return (x**2-y**2+b*x+c),(2*x*y+b*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e7978b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 4/10000 [00:00<05:02, 33.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  0 Loss:  8.840611156438738  lr:  0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▌                            | 2504/10000 [01:14<03:46, 33.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2500 Loss:  0.010104551923155975  lr:  0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████                   | 5004/10000 [02:29<02:29, 33.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  5000 Loss:  0.001461997881196653  lr:  0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████▌         | 7504/10000 [03:43<01:14, 33.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  7500 Loss:  0.0008577223383164048  lr:  0.00016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10000/10000 [04:57<00:00, 33.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss:  0.0003930701666850679\n",
      "total time:  297.43820357322693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2nd order\n",
    "b = np.linspace(-5,5,500)\n",
    "c = np.linspace(-5,5,500)\n",
    "B,C = np.meshgrid(b,c)\n",
    "x_train1 = torch.from_numpy(np.hstack(( B.flatten()[:,None],C.flatten()[:,None]))).to(device)\n",
    "\n",
    "steps=10000\n",
    "layers = np.array([2,50,50,50,50,50,50,2])\n",
    "Root_NN2 = NNCR(layers)\n",
    "Root_NN2.to(device)\n",
    "Root_NN2.closure(steps=steps,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9353a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Root_NN2,'models/Complex_quad.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59004a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/10000 [00:00<?, ?it/s]/home/djc/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([390625, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|                                         | 5/10000 [00:00<07:26, 22.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  0 Loss:  2.4381411504344594  lr:  0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▌                            | 2504/10000 [01:40<05:02, 24.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2500 Loss:  0.000105973121632195  lr:  0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████                   | 5003/10000 [03:21<03:21, 24.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  5000 Loss:  3.646943897663825e-05  lr:  8e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████▌         | 7505/10000 [05:02<01:40, 24.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  7500 Loss:  2.0782449272964285e-05  lr:  1.6000000000000003e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10000/10000 [06:43<00:00, 24.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss:  1.5083995924848818e-05\n",
      "total time:  403.2259407043457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4rd order\n",
    "b = np.linspace(0,2,25)\n",
    "c = np.linspace(0,2,25)\n",
    "d = np.linspace(0,2,25)\n",
    "e = np.linspace(-2,0,25)\n",
    "B,C,D,E = np.meshgrid(b,c,d,e)\n",
    "x_train = torch.from_numpy(np.hstack(( B.flatten()[:,None],C.flatten()[:,None],\n",
    "                                      D.flatten()[:,None],E.flatten()[:,None]))).to(device)\n",
    "\n",
    "steps=10000\n",
    "layers = np.array([4,20,20,20,20,20,20,20,20,1])\n",
    "Root_NN4 = NNR(layers)\n",
    "Root_NN4.to(device)\n",
    "Root_NN4.closure(steps=steps,n=4,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afb8df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Root_NN4,'models/Quartic.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d34bdf",
   "metadata": {},
   "source": [
    "# Tests for all roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5331b18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roots are 0.63578, -1.06777+1.09805i, -1.06777+-1.09805i\n"
     ]
    }
   ],
   "source": [
    "#Cubic1\n",
    "test = np.array([1,1.5,1,-1.5])\n",
    "layers = np.array([3,20,20,20,20,20,20,20,20,1])\n",
    "NN3 = torch.load('models/Cubic.pt')\n",
    "root1 = NN3(test[1:]).detach().cpu().numpy()[0]\n",
    "poly2,rem = poly_div(test,[1,-1*root1])\n",
    "layers = np.array([2,50,50,50,50,50,50,2])\n",
    "NN2 = torch.load('models/Complex_quad.pt')\n",
    "root2 = NN2(np.array(poly2[1:])).detach().cpu().numpy()\n",
    "root3 = [root2[0],-root2[1]]\n",
    "print('roots are %1.5f, %1.5f+%1.5fi, %1.5f+%1.5fi'%(root1,root2[0],root2[1],root3[0],root3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce54bd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roots are 0.37606, -0.43420+1.06800i, -0.43420+-1.06800i\n"
     ]
    }
   ],
   "source": [
    "#Cubic2\n",
    "test = np.array([1,0.5,1,-0.5])\n",
    "\n",
    "layers = np.array([3,20,20,20,20,20,20,20,20,1])\n",
    "NN3 = torch.load('models/Cubic.pt')\n",
    "root1 = NN3(test[1:]).detach().cpu().numpy()[0]\n",
    "poly2,rem = poly_div(test,[1,-1*root1])\n",
    "layers = np.array([2,50,50,50,50,50,50,2])\n",
    "NN2 = torch.load('models/Complex_quad.pt')\n",
    "root2 = NN2(np.array(poly2[1:])).detach().cpu().numpy()\n",
    "root3 = [root2[0],-root2[1]]\n",
    "print('roots are %1.5f, %1.5f+%1.5fi, %1.5f+%1.5fi'%(root1,root2[0],root2[1],root3[0],root3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5d8b9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roots are 0.51839, -1.29037, -0.10957+1.21448i, -0.10957+-1.21448i\n"
     ]
    }
   ],
   "source": [
    "#Quartic\n",
    "test4 =  [1,1,1,1,-1]\n",
    "layers = np.array([4,20,20,20,20,20,20,20,20,1])\n",
    "NN4 = torch.load('models/Quartic.pt')\n",
    "root1 = NN4(np.array(test4[1:])).detach().cpu().numpy()[0]\n",
    "poly3,rem = poly_div(test4,[1,-root1])\n",
    "\n",
    "layers = np.array([3,20,20,20,20,20,20,20,20,1])\n",
    "NN3 = torch.load('models/Cubic.pt')\n",
    "root2 = NN3(np.array(poly3[1:])).detach().cpu().numpy()[0]\n",
    "poly2,rem = poly_div(poly3,[1,-root2])\n",
    "\n",
    "layers = np.array([2,50,50,50,50,50,50,2])\n",
    "NN2 = torch.load('models/Complex_quad.pt')\n",
    "root3 = NN2(np.array(poly2[1:])).detach().cpu().numpy()\n",
    "root4 = [root3[0],-root3[1]]\n",
    "print('roots are %1.5f, %1.5f, %1.5f+%1.5fi, %1.5f+%1.5fi'%(root1,root2,root4[0],root4[1],root3[0],root3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde5759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
